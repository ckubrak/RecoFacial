\subsubsection*{}
Para experimentar analizamos la influencia en los resultados de las diferentes variables de experimentación que manejamos utilizando tanto el método KNN como el KNN + PCA. 
Las mismas son la cantidad de vecinos considerados por KNN (k), y la cantidad de componentes principales de la imagen transformada ($\alpha$).
Lo hicimos sobre dos bases de datos: una con im\'agenes de tamaño reducido, y otra con imágenes sin reducir.
Las métricas que elegimos para evaluar fueron accuracy y precision, con el accuracy tenemos una medida de clasificaciones correctas en general (o sea las veces que se acierta en clasificar en la clase correcta y en no clasificar en una clase que no corresponde), mientras que el precision nos permite minimizar la cantidad de identificaciones desacertadas que tolera el sistema.

\par Nuestras expectativas previas a la experimentación fueron las siguientes\\
Por un lado consideramos que el tamaño de las im\'agenes influiría acrecentando el tiempo requerido para su procesamiento pero debido a la mayor informaci\'on disponible, con las imágenes más grandes 
funcionarían mejor los algoritmos.\\
Con respecto a la variaci\'on del $k$ en $KNN$ suponemos que los mejores resultados de reconocimiento los tendríamos con un $k$ no demasiado grande.\\
Frente a los diferentes algoritmos (KNN o PCA + KNN) creemos que la segunda opci\'on realizar\'a un mejor trabajo pero en un mayor tiempo, por lo que habr\'a que evaluar si se justifica su utilizaci\'on.\\
Por \'ultimo, respecto al par\'ametro $\alpha$ (cantidad de iteraciones del m\'etodo de las potencias) resulta obvio estimar que a mayor $\alpha$, mayor ser\'a el tiempo de ejecución aunque como contrapartida esperamos que un mayor $\alpha$ mejore significativamente la tasa de reconocimiento.

\subsubsection*{Casos de prueba}

\begin{enumerate}
\item La primera experiencia que realizamos fue utilizando las imágenes de la base de datos reducida. La idea de esta experiencia fue evaluar qué cantidad de vecinos daba un mejor resultado de las métricas seleccionadas. Variamos el valor de k de KNN entre 1 y 280, variando de a 20. Los mejores resultados los obtuvimos con k=1.
\end{enumerate}
